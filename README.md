TasteIQ

Personalized restaurant and meal recommendations powered by Retrieval-Augmented Generation (RAG).

ğŸ§  Overview

TasteIQ is a GenAI-driven system that helps users discover and plan meals based on their preferences, dietary goals, and restaurant availability.

Using a Retrieval-Augmented Generation (RAG) pipeline, the system grounds LLM responses in structured nutrition and menu data, enabling accurate, constraint-aware recommendations rather than free-form hallucinations.

Using a RAG-based architecture, TasteIQ combines:

Structured data from the Spoonacular API (menu items, nutrition, recipes, restaurants)

Semantic retrieval over embedded menu items for relevant context selection

GPT-4o for reasoning, personalization, and recommendation synthesis

Cuisine-aware prompt routing for nuanced recommendation styles (e.g. Asian, Mexican, Fast Food)

The result is an intelligent conversational agent that can answer queries like:

â€œFind me a vegan fast-food meal under 600 calories.â€
â€œRecommend high-protein Mexican dishes for post-workout recovery.â€
â€œWhatâ€™s a balanced dinner option from Chipotle today?â€

ğŸ’¡ Why This Matters

Modern food discovery tools are limited to static filters or crowd-sourced ratings, which struggle to handle multiple simultaneous constraints such as nutrition targets, dietary rules, and personal preferences.

TasteIQ moves beyond this by combining nutritional intelligence, semantic retrieval, and LLM-based reasoning to produce grounded, personalized recommendations.

It represents how applied GenAI systems bridge structured data (menus, nutrition) with LLM-driven decision-making â€” an approach increasingly used across real-world recommender systems, digital health platforms, and consumer AI products.

ğŸ§© Core Features
Feature	Description
ğŸ” Restaurant-Aware Search	Integrates with Spoonacularâ€™s restaurant and menu item endpoints to retrieve real fast-food and chain options.
ğŸ§¬ Nutrition Intelligence	Automatically analyzes and ranks meals based on macronutrients, calories, and dietary constraints.
ğŸ§  RAG-Based Question Answering	Uses semantic retrieval to ground LLM responses in factual nutrition and menu data.
ğŸŒ Cuisine-Aware Personalization	Routes queries to cuisine-specific prompt templates for contextually appropriate recommendations.
ğŸ’¬ Conversational AI Interface	GPT-4o handles multi-constraint reasoning and natural dialogue.
â˜ï¸ Cloud-Native Deployment	Containerized with Docker and deployable on AWS ECS/Fargate for scalable serving.
ğŸ§± Planned Architecture
backend/
â”œâ”€â”€ app.py                    # FastAPI backend entrypoint
â”œâ”€â”€ api/                      # API routing layer
â”‚   â”œâ”€â”€ routes.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ services/                 # External service integrations
â”‚   â”œâ”€â”€ spoonacular_api.py
â”‚   â”œâ”€â”€ llm_service.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ database/                 # Data ingestion & query logic
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â””â”€â”€ queries.py
â”œâ”€â”€ rag/                      # RAG pipeline components
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â””â”€â”€ pipeline.py
â”œâ”€â”€ evaluation/               # Retrieval + response evaluation
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ benchmarks.py
â”œâ”€â”€ utils/                    # Helper functions
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â””â”€â”€ helpers.py
â””â”€â”€ tests/                    # Unit and integration tests
    â””â”€â”€ test_api.py

ğŸ” System Design
1ï¸âƒ£ Data Layer

Pulls real-time menu and nutrition data from the Spoonacular API

Normalizes and stores data in a local or cloud database (SQLite â†’ PostgreSQL/RDS)

Enriches menu items with embeddings for semantic retrieval (Weaviate)

2ï¸âƒ£ RAG Pipeline

User queries are embedded and matched against stored menu embeddings

Top-k retrieved menu items are injected into GPT-4o prompt templates

GPT synthesizes responses grounded in retrieved nutritional context

This design significantly reduces hallucinated nutrition facts compared to a prompt-only LLM baseline.

3ï¸âƒ£ Personalization & Model Strategy

Uses prompt routing based on inferred cuisine and dietary intent

Explores lightweight fine-tuning and prompt variants as an experimental comparison, not a core dependency

Prioritizes retrieval quality and prompt structure over heavy model specialization

4ï¸âƒ£ Deployment Layer

Packaged via Docker for reproducible builds

Deployable to AWS ECS/Fargate or Lambda (serverless option)

S3 for data/artifact storage, CloudWatch for logs and metrics

ğŸ§® Example Query Flow

User:

â€œI want a low-carb dinner from a fast-food place.â€

RAG Retriever:
Fetches relevant low-carb menu items from Spoonacular embeddings

GPT-4o Reasoning:
Applies dietary constraints, ranking logic, and preference filters

Response:

â€œTry Grilled Chicken Salad from Chick-fil-A â€” approximately 8g net carbs and 320 calories.â€

ğŸ§° Tech Stack
Layer	Tools
Backend	Python, FastAPI
Model Serving	OpenAI GPT-4o
Data Ingestion	Spoonacular API, Pandas
Database	SQLite / PostgreSQL
Vector Store	Weaviate
Retrieval Augmented Generation	LlamaIndex
MLOps / Deployment	Docker, AWS ECS/Fargate, S3, CloudWatch
Version Control	Git + GitHub Actions
ğŸ§‘â€ğŸ³ Future Enhancements

ğŸ¤– Agent-based extensions (restaurant lookup, ordering workflows)

ğŸ§  Long-term user preference memory

ğŸ“Š Expanded evaluation dashboard for retrieval quality and response correctness

ğŸ’¬ Improved multi-turn dialogue state tracking

ğŸš€ Deployment Plan
Stage	Goal
Phase 1	Local development, Spoonacular ingestion, RAG prototype
Phase 2	GPT-4o integration + API deployment via FastAPI
Phase 3	Evaluation pipeline and prompt routing
Phase 4	Containerize and deploy to AWS ECS
Phase 5	Add automated tests and CI/CD workflow
ğŸ“¦ Installation (Planned)
# Clone the repo
git clone https://github.com/<yourusername>/TasteIQ
cd TasteIQ/backend

# Set up virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env

# Run locally
python app.py
